import streamlit as st
import os
import tempfile
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_pinecone import PineconeVectorStore
from pinecone.grpc import PineconeGRPC as Pinecone
from pinecone import ServerlessSpec
import time

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
st.set_page_config(
    page_title="QA ChatBot",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# –°—Ç–∏–ª–∏
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 1rem 0;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .stButton > button {
        width: 100%;
        border-radius: 20px;
        border: none;
        padding: 0.5rem 1rem;
        background: linear-gradient(45deg, #667eea, #764ba2);
        color: white;
    }
    .success-box {
        padding: 1rem;
        border-radius: 10px;
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è session state
if 'vectorstore' not in st.session_state:
    st.session_state.vectorstore = None
if 'documents_loaded' not in st.session_state:
    st.session_state.documents_loaded = False
if 'messages' not in st.session_state:
    st.session_state.messages = []

@st.cache_resource
def setup_pinecone(api_key):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Pinecone —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    try:
        pc = Pinecone(api_key=api_key)
        index_name = "qa-chatbot-streamlit"
        
        existing_indexes = [idx['name'] for idx in pc.list_indexes().indexes]
        
        if index_name not in existing_indexes:
            with st.spinner("–°–æ–∑–¥–∞—é –∏–Ω–¥–µ–∫—Å –≤ Pinecone..."):
                pc.create_index(
                    name=index_name,
                    dimension=1536,
                    metric='cosine',
                    spec=ServerlessSpec(cloud='aws', region='us-east-1')
                )
                
                while not pc.describe_index(index_name).status['ready']:
                    time.sleep(1)
        
        return pc.Index(index_name)
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Pinecone: {e}")
        return None

def process_document(uploaded_file, openai_key, pinecone_key):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ API –∫–ª—é—á–µ–π
        if not openai_key or not pinecone_key:
            return False, "–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å API –∫–ª—é—á–∏!"
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ PDF
        status_text.text("–ó–∞–≥—Ä—É–∂–∞—é PDF...")
        progress_bar.progress(20)
        
        loader = PyPDFLoader(tmp_file_path)
        pages = loader.load_and_split()
        
        if not pages:
            return False, "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ PDF"
        
        # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
        status_text.text("–†–∞–∑–±–∏–≤–∞—é –Ω–∞ —á–∞—Å—Ç–∏...")
        progress_bar.progress(40)
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100
        )
        splits = text_splitter.split_documents(pages)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Pinecone
        status_text.text("–ù–∞—Å—Ç—Ä–∞–∏–≤–∞—é –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É...")
        progress_bar.progress(60)
        
        index = setup_pinecone(pinecone_key)
        if not index:
            return False, "–û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Pinecone"
        
        # –°–æ–∑–¥–∞–Ω–∏–µ embeddings
        status_text.text("–°–æ–∑–¥–∞—é –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è...")
        progress_bar.progress(80)
        
        embeddings = OpenAIEmbeddings(api_key=openai_key)
        vectorstore = PineconeVectorStore(index, embeddings, "text")
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        status_text.text("–°–æ—Ö—Ä–∞–Ω—è—é –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
        vectorstore.add_documents(splits)
        
        progress_bar.progress(100)
        status_text.text("–ì–æ—Ç–æ–≤–æ!")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ session state
        st.session_state.vectorstore = vectorstore
        st.session_state.documents_loaded = True
        
        # –û—á–∏—Å—Ç–∫–∞
        os.unlink(tmp_file_path)
        
        return True, f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(splits)} —á–∞—Å—Ç–µ–π –∏–∑ {len(pages)} —Å—Ç—Ä–∞–Ω–∏—Ü"
        
    except Exception as e:
        return False, f"‚ùå –û—à–∏–±–∫–∞: {str(e)}"

def answer_question(question, openai_key):
    """–û—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å"""
    try:
        if not st.session_state.vectorstore:
            return "–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç"
        
        retriever = st.session_state.vectorstore.as_retriever(
            search_kwargs={"k": 3}
        )
        
        prompt = PromptTemplate.from_template(
            """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
            
–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {question}

–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
1. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
2. –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏
3. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç —è—Å–Ω–æ –∏ –ª–æ–≥–∏—á–Ω–æ
4. –£–∫–∞–∂–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ

–û—Ç–≤–µ—Ç:"""
        )
        
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.3,
            api_key=openai_key
        )
        
        def format_docs(docs):
            return "\n\n".join([f"–ò—Å—Ç–æ—á–Ω–∏–∫ {i+1}:\n{doc.page_content}" for i, doc in enumerate(docs)])
        
        rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
        )
        
        response = rag_chain.invoke(question)
        return response.content
        
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–≤–µ—Ç–µ: {str(e)}"

# –ó–∞–≥–æ–ª–æ–≤–æ–∫
st.markdown("""
<div class="main-header">
    <h1 style="color: white; margin: 0;">ü§ñ QA ChatBot</h1>
    <p style="color: white; margin: 0; opacity: 0.8;">–ó–∞–≥—Ä—É–∑–∏—Ç–µ PDF –∏ –∑–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é</p>
</div>
""", unsafe_allow_html=True)

# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å
with st.sidebar:
    st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
    
    # API –∫–ª—é—á–∏
    st.subheader("üîë API –ö–ª—é—á–∏")
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–ª—é—á–∏ –∏–∑ secrets –∏–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –≤–≤–æ–¥–∞
    openai_key = st.text_input(
        "OpenAI API Key",
        type="password",
        value=st.secrets.get("OPENAI_API_KEY", "") if hasattr(st, 'secrets') else "",
        help="–í–∞—à API –∫–ª—é—á –æ—Ç OpenAI"
    )
    
    pinecone_key = st.text_input(
        "Pinecone API Key",
        type="password",
        value=st.secrets.get("PINECONE_API_KEY", "") if hasattr(st, 'secrets') else "",
        help="–í–∞—à API –∫–ª—é—á –æ—Ç Pinecone"
    )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–π
    keys_status = st.empty()
    if openai_key and pinecone_key:
        keys_status.success("‚úÖ API –∫–ª—é—á–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
    else:
        keys_status.warning("‚ö†Ô∏è –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å API –∫–ª—é—á–∏")
    
    st.divider()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
    st.subheader("üìÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    
    uploaded_file = st.file_uploader(
        "–í—ã–±–µ—Ä–∏—Ç–µ PDF —Ñ–∞–π–ª",
        type=['pdf'],
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: 10MB"
    )
    
    if uploaded_file:
        st.info(f"üìÑ –§–∞–π–ª: {uploaded_file.name}")
        st.info(f"üìä –†–∞–∑–º–µ—Ä: {uploaded_file.size / 1024:.1f} KB")
        
        if st.button("üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç", key="process_btn"):
            if not openai_key or not pinecone_key:
                st.error("‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å API –∫–ª—é—á–∏!")
            else:
                with st.spinner("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –¥–æ–∫—É–º–µ–Ω—Ç..."):
                    success, message = process_document(uploaded_file, openai_key, pinecone_key)
                    if success:
                        st.success(message)
                        st.balloons()
                    else:
                        st.error(message)
    
    st.divider()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    if st.session_state.documents_loaded:
        st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
        st.metric("–í–æ–ø—Ä–æ—Å–æ–≤ –∑–∞–¥–∞–Ω–æ", len([m for m in st.session_state.messages if m["role"] == "user"]))
        
        if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é"):
            st.session_state.messages = []
            st.rerun()

# –û—Å–Ω–æ–≤–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
if st.session_state.documents_loaded:
    st.markdown("""
    <div class="success-box">
        <h3 style="color: #155724; margin: 0;">‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!</h3>
        <p style="color: #155724; margin: 0;">–ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –≤ —á–∞—Ç–µ –Ω–∏–∂–µ</p>
    </div>
    """, unsafe_allow_html=True)
    
    # –ß–∞—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
    st.subheader("üí¨ –ß–∞—Ç —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º")
    
    # –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏–π
    chat_container = st.container()
    
    with chat_container:
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
    
    # –ü–æ–ª–µ –≤–≤–æ–¥–∞
    if prompt := st.chat_input("–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É..."):
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
        with st.chat_message("assistant"):
            with st.spinner("ü§î –î—É–º–∞—é..."):
                response = answer_question(prompt, openai_key)
            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})

else:
    # –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col2:
        st.markdown("""
        <div style="text-align: center; padding: 2rem;">
            <h2>üëã –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å!</h2>
            <p>–î–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã:</p>
            <ol style="text-align: left;">
                <li>–í–≤–µ–¥–∏—Ç–µ API –∫–ª—é—á–∏ –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏</li>
                <li>–ó–∞–≥—Ä—É–∑–∏—Ç–µ PDF –¥–æ–∫—É–º–µ–Ω—Ç</li>
                <li>–ù–∞–∂–º–∏—Ç–µ "–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç"</li>
                <li>–ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é!</li>
            </ol>
        </div>
        """, unsafe_allow_html=True)
        
        # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        with st.expander("üí° –ü—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤"):
            st.markdown("""
            - "–û —á–µ–º —ç—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç?"
            - "–ö–∞–∫–∏–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã?"
            - "–ß—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç—Å—è –æ [–∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç–µ–º–µ]?"
            - "–ü–µ—Ä–µ—á–∏—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—É–Ω–∫—Ç—ã"
            - "–ö–∞–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è?"
            """)
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ
        with st.expander("‚ÑπÔ∏è –ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç"):
            st.markdown("""
            **QA ChatBot** –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ AI —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏:
            
            1. **–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤**: –í–∞—à PDF —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —á–∞—Å—Ç–∏
            2. **–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è**: –ö–∞–∂–¥–∞—è —á–∞—Å—Ç—å –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –≤ —á–∏—Å–ª–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
            3. **–ü–æ–∏—Å–∫**: –ü—Ä–∏ –≤–æ–ø—Ä–æ—Å–µ —Å–∏—Å—Ç–µ–º–∞ –Ω–∞—Ö–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞—Å—Ç–∏
            4. **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞**: AI —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            
            **–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏:**
            - ü§ñ OpenAI GPT –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤
            - üîç Pinecone –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            - üìÑ LangChain –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            """)

# –§—É—Ç–µ—Ä
st.divider()
col1, col2, col3 = st.columns([1, 2, 1])
with col2:
    st.markdown("""
    <div style='text-align: center; padding: 1rem;'>
        <small>
            ü§ñ QA ChatBot | Powered by OpenAI & Pinecone<br>
            Made with ‚ù§Ô∏è using Streamlit
        </small>
    </div>
    """, unsafe_allow_html=True)

# –°–∫—Ä—ã—Ç–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (—Ç–æ–ª—å–∫–æ –≤ development)
if st.checkbox("üîß Debug Info", value=False):
    st.json({
        "documents_loaded": st.session_state.documents_loaded,
        "vectorstore_exists": st.session_state.vectorstore is not None,
        "messages_count": len(st.session_state.messages),
        "openai_key_set": bool(openai_key),
        "pinecone_key_set": bool(pinecone_key)
    })
